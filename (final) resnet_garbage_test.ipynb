{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(final) garbage_test",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWZlH5xkDudD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import ImageFolder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실행시 등장하는 URL을 클릭하여 허용해주면 인증KEY가 나타난다. 복사하여 URL아래 빈칸에 붙여넣으면 마운트에 성공하게된다.\n",
        "from google.colab import drive\n",
        "drive.mount('./MyDrive')\n"
      ],
      "metadata": {
        "id": "gKxokJBpEWmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchvision "
      ],
      "metadata": {
        "id": "oZrcr5YvFsLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/MyDrive/MyDrive/dataset_AITEAM\"\n",
        "\n",
        "classes = os.listdir(data_dir)\n",
        "print(classes)"
      ],
      "metadata": {
        "id": "MIMWtXZ8EWpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "transformations = transforms.Compose([transforms.Resize((64, 64 )),  transforms.ToTensor()])\n",
        "dataset = ImageFolder(data_dir, transform = transformations)\n",
        "dataset"
      ],
      "metadata": {
        "id": "RVBbhmLQOjmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(img, label):\n",
        "    print(\"Label:\", dataset.classes[label])\n",
        "    plt.imshow(img.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "PIWPDHpmOjnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = dataset[257]\n",
        "show_image(img, label)"
      ],
      "metadata": {
        "id": "5JeYs05dOjrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = dataset[4199]\n",
        "show_image(img, label)"
      ],
      "metadata": {
        "id": "F5URzA3IOjwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "8g0m_SawOwzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 51\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [4000, 1017])\n",
        "len(train_ds), len(val_ds)"
      ],
      "metadata": {
        "id": "PNERDLFYOw1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_ds에 이미지 정보(픽셀별 RGB값이 어떻게 들어가 있는지 확인)\n",
        "\n",
        "print(train_ds[0]) # train_ds에서 첫번째 이미지의 이미지정보와 레이블(0:cardboard)\n",
        "\n",
        "print(train_ds[0][0]) # 첫번째 이미지의 이미지에서 레이블을 제외하고 이미지정보만 가져오기\n",
        "print(len(train_ds[0][0][0])) # R값 > 64개의 픽셀\n",
        "print(len(train_ds[0][0][1])) # G값 > 64개의 픽셀\n",
        "print(len(train_ds[0][0][2])) # B값 > 64개의 픽셀"
      ],
      "metadata": {
        "id": "1MVlVTVoGw8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To normalize the dataset, calculate the mean and std\n",
        "train_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in train_ds]\n",
        "train_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in train_ds]\n",
        "\n",
        "train_meanR = np.mean([m[0] for m in train_meanRGB])\n",
        "train_meanG = np.mean([m[1] for m in train_meanRGB])\n",
        "train_meanB = np.mean([m[2] for m in train_meanRGB])\n",
        "train_stdR = np.mean([s[0] for s in train_stdRGB])\n",
        "train_stdG = np.mean([s[1] for s in train_stdRGB])\n",
        "train_stdB = np.mean([s[2] for s in train_stdRGB])\n"
      ],
      "metadata": {
        "id": "RF3Q94SsHLTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_stdB"
      ],
      "metadata": {
        "id": "_ce0iGzdO7fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in val_ds]\n",
        "val_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in val_ds]\n",
        "\n",
        "val_meanR = np.mean([m[0] for m in val_meanRGB])\n",
        "val_meanG = np.mean([m[1] for m in val_meanRGB])\n",
        "val_meanB = np.mean([m[2] for m in val_meanRGB])\n",
        "\n",
        "val_stdR = np.mean([s[0] for s in val_stdRGB])\n",
        "val_stdG = np.mean([s[1] for s in val_stdRGB])\n",
        "val_stdB = np.mean([s[2] for s in val_stdRGB])\n",
        "\n",
        "print(train_meanR, train_meanG, train_meanB)\n",
        "print(val_meanR, val_meanG, val_meanB)"
      ],
      "metadata": {
        "id": "NJ3PIMQ2HLZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_stdR, train_stdG, train_stdB)\n",
        "print(val_stdR, val_stdG, val_stdB)"
      ],
      "metadata": {
        "id": "dJn_U52NQ2_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  dataset에 적용할 transformation을 정의합니다.\n",
        "# define the image transformation\n",
        "train_transformation = transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Resize(224),\n",
        "                        transforms.Normalize([train_meanR, train_meanG, train_meanB],[train_stdR, train_stdG, train_stdB]),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "])\n",
        "\n",
        "val_transformation = transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Resize(224),\n",
        "                        transforms.Normalize([train_meanR, train_meanG, train_meanB],[train_stdR, train_stdG, train_stdB]),\n",
        "])"
      ],
      "metadata": {
        "id": "rfBsf9G0HRWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformation을 dataset에 적용하고, dataloader를 생성합니다.\n",
        "# apply transforamtion\n",
        "train_ds.transform = train_transformation\n",
        "val_ds.transform = val_transformation\n",
        "\n",
        "# create DataLoader\n",
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "svmanHjVQ86R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install utils"
      ],
      "metadata": {
        "id": "_P850n5URVR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import *"
      ],
      "metadata": {
        "id": "sCSgHDiNReTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo"
      ],
      "metadata": {
        "id": "9YvJPb0-HRrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "    \n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x) # 3x3 stride = 2\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out) # 3x3 stride = 1\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "APIDhfkJR3Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes) #conv1x1(64,64)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)#conv3x3(64,64)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion) #conv1x1(64,256)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x) # 1x1 stride = 1\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out) # 3x3 stride = stride \n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out) # 1x1 stride = 1\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "XOoDr_eZXT0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  # model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) #resnet 50\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        \n",
        "        self.inplanes = 64     \n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)   \n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)  \n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  \n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)     \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "    \n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None  \n",
        "        if stride != 1 or self.inplanes != planes * block.expansion: \n",
        "            \n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride), #conv1x1(256, 512, 2)\n",
        "                nn.BatchNorm2d(planes * block.expansion), #batchnrom2d(512)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))       \n",
        "        self.inplanes = planes * block.expansion #self.inplanes = 128 * 4\n",
        "        \n",
        "        for _ in range(1, blocks): \n",
        "            layers.append(block(self.inplanes, planes)) # * 3\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "yoHz_vwnR3aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18(pretrained=False, **kwargs):\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs) #=> 2*(2+2+2+2) +1(conv1) +1(fc)  = 16 +2 =resnet 18\n",
        "    return model\n",
        "\n",
        "def resnet34(pretrained=False, **kwargs):\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "    \n",
        "\n",
        "def resnet50(pretrained=False, **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) #=> 3*(3+4+6+3) +(conv1) +1(fc) = 48 +2 = 50\n",
        "    return model\n",
        "\n",
        "def resnet101(pretrained=False, **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs) \n",
        "    return model\n",
        "    \n",
        "def resnet152(pretrained=False, **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs) # 3*(3+8+36+3) +2 = 150+2 = resnet152    \n",
        "    return model"
      ],
      "metadata": {
        "id": "p7q1-_aqSYQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = resnet50().to(device)\n",
        "x = torch.randn(3, 3, 224, 224).to(device)\n",
        "output = model(x)"
      ],
      "metadata": {
        "id": "mL9L4JZXSYc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, (3, 224, 224), device=device.type)"
      ],
      "metadata": {
        "id": "YSKbl78tSYg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습 진행\n",
        "\n",
        "1.  손실 함수, optimizer, lr_scheduler를 정의합니다.\n"
      ],
      "metadata": {
        "id": "AvwWp4DaEz2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "8wDwKqfqEXff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
        "opt = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)"
      ],
      "metadata": {
        "id": "CG_4tr6lEXiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #현재 lr을 계산하는 함수를 정의\n",
        "\n",
        "# function to get current lr\n",
        "def get_lr(opt):\n",
        "    for param_group in opt.param_groups:\n",
        "        return param_group['lr']"
      ],
      "metadata": {
        "id": "CDISY1clEXlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#배치당 loss와 metric을 계산하는 함수를 정의합니다.\n",
        "\n",
        "# function to calculate metric per mini-batch\n",
        "def metric_batch(output, target):\n",
        "    pred = output.argmax(1, keepdim=True)\n",
        "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
        "    return corrects\n",
        "\n",
        "# function to calculate loss per mini-batch\n",
        "def loss_batch(loss_func, output, target, opt=None):\n",
        "    loss = loss_func(output, target)\n",
        "    metric_b = metric_batch(output, target)\n",
        "\n",
        "    if opt is not None:\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    return loss.item(), metric_b"
      ],
      "metadata": {
        "id": "LbHNS6JFEXnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #epoch당 loss를 정의하는 함수입니다.\n",
        "\n",
        "# function to calculate loss and metric per epoch\n",
        "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
        "    running_loss = 0.0\n",
        "    running_metric = 0.0\n",
        "    len_data = len(dataset_dl.dataset)\n",
        "\n",
        "    for xb, yb in dataset_dl:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        output = model(xb)\n",
        "\n",
        "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
        "        running_loss += loss_b\n",
        "        \n",
        "        if metric_b is not None:\n",
        "            running_metric += metric_b\n",
        "        \n",
        "        if sanity_check is True:\n",
        "            break\n",
        "\n",
        "    loss = running_loss / len_data\n",
        "    metric = running_metric / len_data\n",
        "    return loss, metric"
      ],
      "metadata": {
        "id": "Tg99GEOJEXqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to start training\n",
        "def train_val(model, params):\n",
        "    num_epochs=params['num_epochs']\n",
        "    loss_func=params[\"loss_func\"]\n",
        "    opt=params[\"optimizer\"]\n",
        "    train_dl=params[\"train_dl\"]\n",
        "    val_dl=params[\"val_dl\"]\n",
        "    sanity_check=params[\"sanity_check\"]\n",
        "    lr_scheduler=params[\"lr_scheduler\"]\n",
        "    path2weights=params[\"path2weights\"]\n",
        "    loss_history = {'train': [], 'val': []}\n",
        "    metric_history = {'train': [], 'val': []}\n",
        "    best_loss = float('inf')\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        current_lr = get_lr(opt)\n",
        "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs-1, current_lr))\n",
        "        model.train()\n",
        "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        metric_history['train'].append(train_metric)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
        "        loss_history['val'].append(val_loss)\n",
        "        metric_history['val'].append(val_metric)\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            print('Get best val_loss')\n",
        "        lr_scheduler.step(val_loss)\n",
        "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
        "        print('-'*10)\n",
        "    return model, loss_history, metric_history"
      ],
      "metadata": {
        "id": "doJynGAWEXsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# definc the training parameters\n",
        "params_train = {\n",
        "    'num_epochs':20,\n",
        "    'optimizer':opt,\n",
        "    'loss_func':loss_func,\n",
        "    'train_dl':train_dl,\n",
        "    'val_dl':val_dl,\n",
        "    'sanity_check':False,\n",
        "    'lr_scheduler':lr_scheduler,\n",
        "    'path2weights':'./models/weights.pt',\n",
        "}\n",
        "\n",
        "# create the directory that stores weights.pt\n",
        "def createFolder(directory):\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "    except OSerror:\n",
        "        print('Error')\n",
        "createFolder('./models')"
      ],
      "metadata": {
        "id": "7JTC7wHYEXyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "0zakc5BjF43O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, loss_hist, metric_hist = train_val(model, params_train)"
      ],
      "metadata": {
        "id": "2g_Iyj9VEX1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train loss, val loss 그래프와 정확도 그래프를 확인하겠습니다.\n",
        "\n",
        "# Train-Validation Progress\n",
        "num_epochs=params_train[\"num_epochs\"]\n",
        "\n",
        "# plot loss progress\n",
        "plt.title(\"Train-Val Loss\")\n",
        "plt.plot(range(1,num_epochs+1),loss_hist[\"train\"],label=\"train\")\n",
        "plt.plot(range(1,num_epochs+1),loss_hist[\"val\"],label=\"val\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy progress\n",
        "plt.title(\"Train-Val Accuracy\")\n",
        "plt.plot(range(1,num_epochs+1),metric_hist[\"train\"],label=\"train\")\n",
        "plt.plot(range(1,num_epochs+1),metric_hist[\"val\"],label=\"val\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Id672PcQ9DAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(img, model):\n",
        "    # Convert to a batch of 1\n",
        "    xb = img.unsqueeze(0)\n",
        "    # Get predictions from model\n",
        "    yb = model(xb)\n",
        "    # Pick index with highest probability\n",
        "    prob, preds  = torch.max(yb, dim=1)\n",
        "    # Retrieve the class label\n",
        "    return dataset.classes[preds[0].item()]"
      ],
      "metadata": {
        "id": "rmlzWQ4vy4QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = train_ds[0]\n",
        "plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
        "print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"
      ],
      "metadata": {
        "id": "Zsjj1b3Ly7b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = test_ds[312]\n",
        "plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
        "print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"
      ],
      "metadata": {
        "id": "_-1nF9hwy9zd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
